{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DreamBooth for MNIST Digit Generation\n",
    "\n",
    "This notebook explores **DreamBooth**, a powerful fine-tuning technique for diffusion models, adapted for MNIST digit generation.\n",
    "\n",
    "## What is DreamBooth?\n",
    "\n",
    "**DreamBooth** is a training technique that teaches a diffusion model to generate images of a specific subject using only a few example images (3-5 typically). The key innovation is:\n",
    "\n",
    "1. **Unique Identifier**: You bind a special token (e.g., `[sks]`) to your subject in text prompts\n",
    "2. **Full Model Fine-tuning**: Unlike LoRA or textual inversion, DreamBooth updates the entire UNet (and optionally the text encoder)\n",
    "3. **Prior Preservation**: Uses the model's own generated images of the general class to prevent overfitting and language drift\n",
    "\n",
    "### How it works:\n",
    "- **Instance Prompt**: `\"a photo of sks dog\"` (where `sks` is your unique identifier)\n",
    "- **Class Prompt**: `\"a photo of a dog\"` (the general class)\n",
    "- **Loss Function**: Combines reconstruction loss on your images + prior preservation loss on class images\n",
    "\n",
    "### For MNIST:\n",
    "We'll adapt this to teach a model to generate specific styles/variations of digits. For example:\n",
    "- Train on a few examples of a specific handwriting style\n",
    "- Generate new digits in that style\n",
    "\n",
    "**References:**\n",
    "- [HuggingFace DreamBooth Docs](https://huggingface.co/docs/diffusers/en/training/dreambooth)\n",
    "- [Original Paper](https://arxiv.org/abs/2208.12242)\n",
    "- [train_dreambooth.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "# Diffusers imports\n",
    "from diffusers import DDPMScheduler, UNet2DModel, DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare MNIST Dataset\n",
    "\n",
    "We'll load MNIST and extract a few examples of a specific digit to use as our \"instance\" images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from the default huggingface cache\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"~/.cache/huggingface/datasets/MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root=\"~/.cache/huggingface/datasets/MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(mnist_train)}\")\n",
    "print(f\"Test samples: {len(mnist_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Instance Images\n",
    "\n",
    "Let's pick a specific digit (e.g., digit \"3\") and select a few instances to represent our \"subject\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TARGET_DIGIT = 3  # The digit we want to personalize\n",
    "NUM_INSTANCE_IMAGES = 5  # How many examples to use (DreamBooth typically uses 3-5)\n",
    "IMAGE_SIZE = 32  # We'll resize MNIST to 32x32 for compatibility with standard diffusion models\n",
    "\n",
    "# Find instances of our target digit\n",
    "instance_indices = [i for i, (img, label) in enumerate(mnist_train) if label == TARGET_DIGIT][:NUM_INSTANCE_IMAGES]\n",
    "instance_images = [mnist_train[i][0] for i in instance_indices]\n",
    "\n",
    "# Visualize our instance images\n",
    "fig, axes = plt.subplots(1, NUM_INSTANCE_IMAGES, figsize=(15, 3))\n",
    "fig.suptitle(f'Instance Images: Digit {TARGET_DIGIT} (Our \"Subject\")', fontsize=14, fontweight='bold')\n",
    "for idx, (ax, img) in enumerate(zip(axes, instance_images)):\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Instance {idx+1}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Class Images\n",
    "\n",
    "For prior preservation, we need images of the general class (all digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS_IMAGES = 100  # Number of class images for prior preservation\n",
    "\n",
    "# Get random images from all digits (excluding our specific instances)\n",
    "class_indices = [i for i in range(len(mnist_train)) if i not in instance_indices][:NUM_CLASS_IMAGES]\n",
    "class_images = [mnist_train[i][0] for i in class_indices]\n",
    "class_labels = [mnist_train[i][1] for i in class_indices]\n",
    "\n",
    "# Visualize some class images\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "fig.suptitle('Class Images: General Digits (For Prior Preservation)', fontsize=14, fontweight='bold')\n",
    "for idx, (ax, img, label) in enumerate(zip(axes.flat, class_images[:20], class_labels[:20])):\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create DreamBooth Dataset\n",
    "\n",
    "We'll create a custom dataset that:\n",
    "1. Returns instance images with an \"instance prompt\"\n",
    "2. Returns class images with a \"class prompt\"\n",
    "3. Preprocesses images to the right size and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DreamBooth training.\n",
    "    \n",
    "    Combines instance images (your specific subject) with class images (prior preservation).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_images,\n",
    "        class_images,\n",
    "        instance_prompt,\n",
    "        class_prompt,\n",
    "        size=32,\n",
    "        repeats=1  # Repeat instance images to balance dataset\n",
    "    ):\n",
    "        self.instance_images = instance_images * repeats  # Repeat to balance with class images\n",
    "        self.class_images = class_images\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.class_prompt = class_prompt\n",
    "        \n",
    "        # Image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # Data augmentation\n",
    "            transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "        # Total length is instance + class images\n",
    "        self.num_instance_images = len(self.instance_images)\n",
    "        self.num_class_images = len(self.class_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(self.num_instance_images, self.num_class_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = {}\n",
    "        \n",
    "        # Get instance image and prompt\n",
    "        instance_idx = idx % self.num_instance_images\n",
    "        instance_image = self.instance_images[instance_idx]\n",
    "        instance_image = self.transform(instance_image)\n",
    "        example[\"instance_images\"] = instance_image\n",
    "        example[\"instance_prompt\"] = self.instance_prompt\n",
    "        \n",
    "        # Get class image and prompt (for prior preservation)\n",
    "        class_idx = idx % self.num_class_images\n",
    "        class_image = self.class_images[class_idx]\n",
    "        class_image = self.transform(class_image)\n",
    "        example[\"class_images\"] = class_image\n",
    "        example[\"class_prompt\"] = self.class_prompt\n",
    "        \n",
    "        return example\n",
    "\n",
    "# Create dataset\n",
    "# Note: For MNIST (unconditional generation), we'll use simpler prompts or class labels\n",
    "instance_prompt = f\"digit_{TARGET_DIGIT}_sks\"  # 'sks' is our unique identifier\n",
    "class_prompt = \"digit\"\n",
    "\n",
    "# Calculate repeats to balance instance and class images\n",
    "repeats = math.ceil(NUM_CLASS_IMAGES / NUM_INSTANCE_IMAGES)\n",
    "\n",
    "dreambooth_dataset = DreamBoothDataset(\n",
    "    instance_images=instance_images,\n",
    "    class_images=class_images,\n",
    "    instance_prompt=instance_prompt,\n",
    "    class_prompt=class_prompt,\n",
    "    size=IMAGE_SIZE,\n",
    "    repeats=repeats\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dreambooth_dataset)}\")\n",
    "print(f\"Instance prompt: '{instance_prompt}'\")\n",
    "print(f\"Class prompt: '{class_prompt}'\")\n",
    "print(f\"Instance images repeated {repeats}x to balance with class images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataset\n",
    "sample = dreambooth_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Instance image shape:\", sample[\"instance_images\"].shape)\n",
    "print(\"Class image shape:\", sample[\"class_images\"].shape)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(sample[\"instance_images\"].squeeze() * 0.5 + 0.5, cmap='gray')  # Denormalize\n",
    "axes[0].set_title(f'Instance: {sample[\"instance_prompt\"]}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sample[\"class_images\"].squeeze() * 0.5 + 0.5, cmap='gray')\n",
    "axes[1].set_title(f'Class: {sample[\"class_prompt\"]}')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Diffusion Model\n",
    "\n",
    "We'll use a simple UNet2D model from diffusers. For a real application, you'd start with a pretrained model and fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model = UNet2DModel(\n",
    "    sample_size=IMAGE_SIZE,\n",
    "    in_channels=1,  # Grayscale\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Noise scheduler (DDPM)\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_schedule=\"squaredcos_cap_v2\"  # Good for small images\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Load a Pretrained Model\n",
    "\n",
    "For better results, you could start with a pretrained model. Uncomment to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained model (optional - requires fine-tuning)\n",
    "# from diffusers import DDPMPipeline\n",
    "# \n",
    "# # Load a small pretrained model (e.g., trained on CIFAR-10)\n",
    "# pretrained_pipeline = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "# model = pretrained_pipeline.unet\n",
    "# noise_scheduler = pretrained_pipeline.scheduler\n",
    "# \n",
    "# # Adapt for grayscale (MNIST)\n",
    "# # Note: This requires modifying the first conv layer\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DreamBooth Training Loop\n",
    "\n",
    "The key to DreamBooth is the **combined loss**:\n",
    "\n",
    "```\n",
    "total_loss = instance_loss + prior_preservation_weight * class_loss\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `instance_loss`: Reconstruction loss on your specific subject images\n",
    "- `class_loss`: Regularization loss on general class images (prevents overfitting)\n",
    "- `prior_preservation_weight`: Controls strength of regularization (typically 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"lr_warmup_steps\": 50,\n",
    "    \"prior_loss_weight\": 1.0,  # Weight for prior preservation loss\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"mixed_precision\": \"no\",  # \"fp16\" if using CUDA and want faster training\n",
    "    \"save_model_epochs\": 20,\n",
    "    \"sample_every_n_epochs\": 10,  # Generate samples during training\n",
    "}\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    dreambooth_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Increase if you have CPU cores available\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config[\"lr_warmup_steps\"],\n",
    "    num_training_steps=len(train_dataloader) * config[\"num_epochs\"],\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nSteps per epoch: {len(train_dataloader)}\")\n",
    "print(f\"Total training steps: {len(train_dataloader) * config['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_dreambooth(\n",
    "    model,\n",
    "    noise_scheduler,\n",
    "    train_dataloader,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    config,\n",
    "    device\n",
    "):\n",
    "    \"\"\"DreamBooth training loop with prior preservation.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        range(config[\"num_epochs\"] * len(train_dataloader)),\n",
    "        desc=\"Training\"\n",
    "    )\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Get instance and class images\n",
    "            instance_images = batch[\"instance_images\"].to(device)\n",
    "            class_images = batch[\"class_images\"].to(device)\n",
    "            \n",
    "            # Combine into single batch for efficiency\n",
    "            images = torch.cat([instance_images, class_images], dim=0)\n",
    "            batch_size = images.shape[0]\n",
    "            \n",
    "            # Sample random timesteps for each image\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (batch_size,),\n",
    "                device=device\n",
    "            ).long()\n",
    "            \n",
    "            # Add noise to images (forward diffusion process)\n",
    "            noise = torch.randn_like(images)\n",
    "            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
    "            \n",
    "            # Predict the noise\n",
    "            noise_pred = model(noisy_images, timesteps).sample\n",
    "            \n",
    "            # Calculate loss\n",
    "            # Split predictions back into instance and class\n",
    "            instance_pred = noise_pred[:len(instance_images)]\n",
    "            class_pred = noise_pred[len(instance_images):]\n",
    "            \n",
    "            instance_noise = noise[:len(instance_images)]\n",
    "            class_noise = noise[len(instance_images):]\n",
    "            \n",
    "            # Instance loss (MSE between predicted and actual noise)\n",
    "            instance_loss = F.mse_loss(instance_pred, instance_noise, reduction=\"mean\")\n",
    "            \n",
    "            # Prior preservation loss (regularization)\n",
    "            class_loss = F.mse_loss(class_pred, class_noise, reduction=\"mean\")\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = instance_loss + config[\"prior_loss_weight\"] * class_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Logging\n",
    "            epoch_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"loss\": loss.item(),\n",
    "                \"inst_loss\": instance_loss.item(),\n",
    "                \"class_loss\": class_loss.item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Generate samples\n",
    "        if (epoch + 1) % config[\"sample_every_n_epochs\"] == 0:\n",
    "            print(f\"Generating samples at epoch {epoch + 1}...\")\n",
    "            generate_samples(model, noise_scheduler, device, num_samples=8)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return losses\n",
    "\n",
    "def generate_samples(model, noise_scheduler, device, num_samples=8):\n",
    "    \"\"\"Generate samples from the model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Start from random noise\n",
    "        images = torch.randn(\n",
    "            num_samples, 1, IMAGE_SIZE, IMAGE_SIZE,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Denoise iteratively\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            # Predict noise\n",
    "            noise_pred = model(images, t).sample\n",
    "            \n",
    "            # Remove noise\n",
    "            images = noise_scheduler.step(noise_pred, t, images).prev_sample\n",
    "        \n",
    "        # Denormalize and clamp\n",
    "        images = (images / 2 + 0.5).clamp(0, 1)\n",
    "        images = images.cpu()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(16, 2))\n",
    "    for idx, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img.squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('Generated Samples', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "print(\"Training functions defined. Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "**Note**: Training can take a while depending on your hardware. Start with fewer epochs for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"Starting DreamBooth training...\\n\")\n",
    "losses = train_dreambooth(\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    train_dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    config=config,\n",
    "    device=device\n",
    ")\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(losses, alpha=0.3, label='Step loss')\n",
    "\n",
    "# Moving average for smoother visualization\n",
    "window_size = 10\n",
    "if len(losses) >= window_size:\n",
    "    moving_avg = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(losses)), moving_avg, linewidth=2, label=f'{window_size}-step moving avg')\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DreamBooth Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate and Evaluate Results\n",
    "\n",
    "Now let's generate images and compare them to our original instance images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of samples\n",
    "num_samples = 16\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Start from random noise\n",
    "    generated_images = torch.randn(\n",
    "        num_samples, 1, IMAGE_SIZE, IMAGE_SIZE,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Denoise with progress bar\n",
    "    for t in tqdm(noise_scheduler.timesteps, desc=\"Generating samples\"):\n",
    "        noise_pred = model(generated_images, t).sample\n",
    "        generated_images = noise_scheduler.step(noise_pred, t, generated_images).prev_sample\n",
    "    \n",
    "    # Denormalize\n",
    "    generated_images = (generated_images / 2 + 0.5).clamp(0, 1)\n",
    "    generated_images = generated_images.cpu()\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "fig.suptitle(f'Generated Digits (Fine-tuned on Digit {TARGET_DIGIT})', fontsize=16, fontweight='bold')\n",
    "for idx, (ax, img) in enumerate(zip(axes.flat, generated_images)):\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Sample {idx+1}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Original Instance Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(2, NUM_INSTANCE_IMAGES, figsize=(15, 6))\n",
    "fig.suptitle(f'Comparison: Original Instance Images vs Generated', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Original instances\n",
    "for idx in range(NUM_INSTANCE_IMAGES):\n",
    "    axes[0, idx].imshow(instance_images[idx].squeeze(), cmap='gray')\n",
    "    axes[0, idx].set_title(f'Original {idx+1}')\n",
    "    axes[0, idx].axis('off')\n",
    "\n",
    "# Generated samples\n",
    "for idx in range(NUM_INSTANCE_IMAGES):\n",
    "    axes[1, idx].imshow(generated_images[idx].squeeze(), cmap='gray')\n",
    "    axes[1, idx].set_title(f'Generated {idx+1}')\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = Path(\"./dreambooth_mnist_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save using diffusers format\n",
    "pipeline = DDPMPipeline(\n",
    "    unet=model,\n",
    "    scheduler=noise_scheduler\n",
    ")\n",
    "pipeline.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# To load later:\n",
    "# loaded_pipeline = DDPMPipeline.from_pretrained(output_dir)\n",
    "# generated = loaded_pipeline(batch_size=8, num_inference_steps=1000).images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiments and Analysis\n",
    "\n",
    "### Experiment 1: Effect of Prior Preservation Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prior preservation weights\n",
    "# Higher weight = stronger regularization = less overfitting but potentially less similarity to instances\n",
    "# Lower weight = weaker regularization = more overfitting but potentially more similarity to instances\n",
    "\n",
    "print(\"\"\"\n",
    "Experiment: Try different prior_loss_weight values:\n",
    "- 0.0: No prior preservation (pure fine-tuning on instances) - may overfit\n",
    "- 0.5: Weak regularization\n",
    "- 1.0: Balanced (default)\n",
    "- 2.0: Strong regularization - may not learn instance well\n",
    "\n",
    "Modify config['prior_loss_weight'] above and re-run training to compare results!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Number of Instance Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Experiment: Try different numbers of instance images:\n",
    "- 3-5 images: Standard DreamBooth setup\n",
    "- 1-2 images: Extreme few-shot learning\n",
    "- 10+ images: More training data (may not need DreamBooth)\n",
    "\n",
    "Modify NUM_INSTANCE_IMAGES above and re-run from dataset creation!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Different Target Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Experiment: Try different target digits:\n",
    "- Simple digits (0, 1): May be easier to learn\n",
    "- Complex digits (8, 9): More variation in handwriting\n",
    "\n",
    "Modify TARGET_DIGIT above and re-run!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **DreamBooth Core Idea**: Fine-tune a diffusion model on a few images of a specific subject using a unique identifier token\n",
    "\n",
    "2. **Prior Preservation**: Essential to prevent overfitting and \"language drift\" - maintains the model's general knowledge\n",
    "\n",
    "3. **Loss Function**:\n",
    "   ```\n",
    "   L = L_instance + λ * L_class\n",
    "   ```\n",
    "   Where λ controls the regularization strength\n",
    "\n",
    "4. **Few-Shot Learning**: DreamBooth excels with just 3-5 training examples\n",
    "\n",
    "5. **Adaptation for MNIST**: While DreamBooth is typically used with text-conditioned models (like Stable Diffusion), the core technique applies to unconditional generation too\n",
    "\n",
    "### Limitations & Next Steps:\n",
    "\n",
    "- **Memory intensive**: Full model fine-tuning requires significant GPU memory\n",
    "- **Overfitting risk**: Too few class images or too many epochs can lead to overfitting\n",
    "- **Hyperparameter sensitivity**: prior_loss_weight, learning rate, and number of training steps all matter\n",
    "\n",
    "### Extensions:\n",
    "\n",
    "1. **Use with Stable Diffusion**: Apply to real images with text conditioning\n",
    "2. **LoRA + DreamBooth**: Combine with Low-Rank Adaptation for efficiency\n",
    "3. **Custom regularization images**: Generate better class images for your specific use case\n",
    "4. **Multi-subject training**: Train on multiple subjects with different identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [DreamBooth Paper](https://arxiv.org/abs/2208.12242)\n",
    "- [HuggingFace DreamBooth Documentation](https://huggingface.co/docs/diffusers/en/training/dreambooth)\n",
    "- [HuggingFace train_dreambooth.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py)\n",
    "- [Diffusers Library](https://github.com/huggingface/diffusers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
