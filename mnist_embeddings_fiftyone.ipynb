{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Embeddings with FiftyOne - MNIST Edition\n",
    "\n",
    "This notebook explores image embeddings using FiftyOne, following the pattern from the [official tutorial](https://docs.voxel51.com/tutorials/image_embeddings.html) but adapted for MNIST.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Loading MNIST dataset into FiftyOne\n",
    "2. Computing embeddings (raw pixels and neural network-based)\n",
    "3. Visualizing embeddings with UMAP, t-SNE, and PCA\n",
    "4. Interactive exploration and analysis\n",
    "5. Finding outliers and potential mislabeled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to launch FiftyOne App in browser\n",
    "def launch_in_browser(dataset_or_view):\n",
    "    \"\"\"Launch FiftyOne App and print URL for browser access\"\"\"\n",
    "    session = fo.launch_app(dataset_or_view, auto=False)\n",
    "    print(f\"\\nüåê Open FiftyOne App in browser:\")\n",
    "    print(f\"   {session.url}\")\n",
    "    print(f\"\\nOr just visit: http://localhost:5151\\n\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset\n",
    "\n",
    "FiftyOne has MNIST built-in through its dataset zoo. We'll load a subset for faster experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST test set (or specify split=\"train\" for training set)\n",
    "# Using max_samples to keep it manageable\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"mnist\",\n",
    "    split=\"test\",\n",
    "    max_samples=5000,  # Adjust as needed\n",
    "    dataset_name=\"mnist_embeddings_tutorial\",\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples\")\n",
    "print(f\"Dataset info: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the FiftyOne App to explore the dataset\n",
    "# session = fo.launch_app(dataset)\n",
    "\n",
    "# To open in browser instead of embedded, use:\n",
    "session = launch_in_browser(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing Embeddings - Method 1: Raw Pixels\n",
    "\n",
    "Since MNIST images are small (28x28), we can use raw pixel values as embeddings. This is simple but can still reveal interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw pixel embeddings\n",
    "# For MNIST (28x28 grayscale), each image becomes a 784-dimensional vector\n",
    "\n",
    "embeddings = []\n",
    "for sample in dataset:\n",
    "    # Load image and flatten to 1D array\n",
    "    img = Image.open(sample.filepath).convert('L')  # Ensure grayscale\n",
    "    img_array = np.array(img).flatten()\n",
    "    embeddings.append(img_array)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Raw pixel embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in the App - embeddings panel should appear\n",
    "# session = fo.launch_app(dataset)\n",
    "# session = launch_in_browser(dataset)\n",
    "\n",
    "# Select the embeddings visualization from the App sidebar\n",
    "# You should see clusters corresponding to different digits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing Embeddings - Method 2: Neural Network Features (MobileNet)\n",
    "\n",
    "Now let's use a pre-trained neural network to compute more sophisticated embeddings. We'll use FiftyOne's Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model from FiftyOne's zoo\n",
    "# MobileNet is lightweight and good for getting started\n",
    "model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "\n",
    "print(f\"Loaded model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings using the model\n",
    "# FiftyOne will extract features from the penultimate layer\n",
    "embeddings_nn = dataset.compute_embeddings(model, batch_size=32)\n",
    "\n",
    "print(f\"Neural network embeddings shape: {embeddings_nn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UMAP visualization for neural network embeddings\n",
    "results_nn = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings_nn,\n",
    "    brain_key=\"mobilenet_umap\",\n",
    "    method=\"umap\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Neural network embeddings visualization ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Dimensionality Reduction Methods\n",
    "\n",
    "Let's compute visualizations using different methods: UMAP, t-SNE, and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t-SNE visualization\n",
    "# results_tsne = fob.compute_visualization(\n",
    "#     dataset,\n",
    "#     embeddings=embeddings_nn,\n",
    "#     brain_key=\"mobilenet_tsne\",\n",
    "#     method=\"tsne\",\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# print(\"t-SNE visualization computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA visualization\n",
    "# results_pca = fob.compute_visualization(\n",
    "#     dataset,\n",
    "#     embeddings=embeddings_nn,\n",
    "#     brain_key=\"mobilenet_pca\",\n",
    "#     method=\"pca\",\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# print(\"PCA visualization computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all available visualizations\n",
    "# print(\"Available embeddings visualizations:\")\n",
    "# for key in dataset.list_brain_runs():\n",
    "#     print(f\"  - {key}\")\n",
    "\n",
    "# # You can switch between them in the App's embeddings panel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Exploration\n",
    "\n",
    "**In the FiftyOne App:**\n",
    "- Use the **Box Select** or **Lasso Select** tools to select clusters\n",
    "- Selected samples will appear in the grid view\n",
    "- Color by `ground_truth.label` to see how digits cluster\n",
    "- Look for:\n",
    "  - **Tight clusters**: Well-separated digit classes\n",
    "  - **Outliers**: Unusual samples that don't fit their cluster\n",
    "  - **Mixed clusters**: Potential mislabeling or ambiguous digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Programmatic analysis: Find samples far from their cluster centers\n",
    "# # This can help identify outliers\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Group embeddings by label\n",
    "# label_to_embeddings = defaultdict(list)\n",
    "# label_to_ids = defaultdict(list)\n",
    "\n",
    "# for sample, embedding in zip(dataset, embeddings_nn):\n",
    "#     label = sample.ground_truth.label\n",
    "#     label_to_embeddings[label].append(embedding)\n",
    "#     label_to_ids[label].append(sample.id)\n",
    "\n",
    "# # For each label, find outliers (samples far from their cluster)\n",
    "# outlier_ids = []\n",
    "\n",
    "# for label, embs in label_to_embeddings.items():\n",
    "#     if len(embs) < 10:  # Skip if too few samples\n",
    "#         continue\n",
    "    \n",
    "#     embs = np.array(embs)\n",
    "#     ids = label_to_ids[label]\n",
    "    \n",
    "#     # Compute center of cluster\n",
    "#     center = embs.mean(axis=0)\n",
    "    \n",
    "#     # Find samples furthest from center\n",
    "#     distances = np.linalg.norm(embs - center, axis=1)\n",
    "    \n",
    "#     # Get top 5 outliers for this digit\n",
    "#     outlier_indices = np.argsort(distances)[-5:]\n",
    "#     outlier_ids.extend([ids[i] for i in outlier_indices])\n",
    "\n",
    "# print(f\"Found {len(outlier_ids)} potential outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a view of outliers\n",
    "# outliers_view = dataset.select(outlier_ids)\n",
    "\n",
    "# print(f\"Outliers view contains {len(outliers_view)} samples\")\n",
    "# print(\"\\nLaunching App with outliers view...\")\n",
    "\n",
    "# # session = fo.launch_app(view=outliers_view)\n",
    "# session = fo.launch_in_browser(view=outliers_view)\n",
    "\n",
    "# # These are the samples that are most different from their digit class!\n",
    "# # Look for potential mislabeling or unusual handwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Finding Similar Samples\n",
    "\n",
    "Use embeddings to find visually similar samples - useful for data cleaning and understanding your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pick a random sample and find its nearest neighbors\n",
    "# query_sample = dataset.first()\n",
    "# query_id = query_sample.id\n",
    "# query_label = query_sample.ground_truth.label\n",
    "\n",
    "# print(f\"Query sample: ID={query_id}, Label={query_label}\")\n",
    "\n",
    "# # Get query embedding\n",
    "# query_idx = dataset.match({\"id\": query_id}).first().id\n",
    "# sample_ids = [s.id for s in dataset]\n",
    "# query_idx = sample_ids.index(query_id)\n",
    "# query_embedding = embeddings_nn[query_idx].reshape(1, -1)\n",
    "\n",
    "# # Find 10 nearest neighbors\n",
    "# nbrs = NearestNeighbors(n_neighbors=11, metric='cosine').fit(embeddings_nn)\n",
    "# distances, indices = nbrs.kneighbors(query_embedding)\n",
    "\n",
    "# # Get neighbor IDs (skip first one as it's the query itself)\n",
    "# neighbor_ids = [sample_ids[i] for i in indices[0][1:]]\n",
    "\n",
    "# print(f\"\\nFound {len(neighbor_ids)} similar samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View the query sample and its neighbors\n",
    "# similar_view = dataset.select([query_id] + neighbor_ids)\n",
    "\n",
    "# # session = fo.launch_app(view=similar_view)\n",
    "# session = fo.launch_in_browser(view=similar_view)\n",
    "\n",
    "# # Notice how visually similar these samples are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Label Quality Investigation\n",
    "\n",
    "Find potential mislabeled samples by looking for samples whose nearest neighbors have different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For each sample, check if its neighbors have the same label\n",
    "# k = 10  # Number of neighbors to check\n",
    "# nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(embeddings_nn)\n",
    "\n",
    "# potential_errors = []\n",
    "\n",
    "# for idx, (sample, embedding) in enumerate(zip(dataset, embeddings_nn)):\n",
    "#     # Find neighbors\n",
    "#     distances, indices = nbrs.kneighbors(embedding.reshape(1, -1))\n",
    "    \n",
    "#     # Get neighbor labels (skip self)\n",
    "#     neighbor_labels = [dataset[sample_ids[i]].ground_truth.label \n",
    "#                       for i in indices[0][1:]]\n",
    "    \n",
    "#     # Check if majority of neighbors have different label\n",
    "#     sample_label = sample.ground_truth.label\n",
    "#     different_count = sum(1 for l in neighbor_labels if l != sample_label)\n",
    "    \n",
    "#     if different_count > k * 0.6:  # More than 60% different\n",
    "#         potential_errors.append({\n",
    "#             'id': sample.id,\n",
    "#             'label': sample_label,\n",
    "#             'common_neighbor_label': max(set(neighbor_labels), \n",
    "#                                         key=neighbor_labels.count),\n",
    "#             'different_ratio': different_count / k\n",
    "#         })\n",
    "\n",
    "# print(f\"Found {len(potential_errors)} potential labeling errors\")\n",
    "\n",
    "# # Show first few\n",
    "# for error in potential_errors[:5]:\n",
    "#     print(f\"  Sample labeled '{error['label']}' but neighbors are mostly '{error['common_neighbor_label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View potential labeling errors\n",
    "# if potential_errors:\n",
    "#     error_ids = [e['id'] for e in potential_errors]\n",
    "#     errors_view = dataset.select(error_ids)\n",
    "    \n",
    "#     # session = fo.launch_app(view=errors_view)\n",
    "#     session = fo.launch_in_browser(view=errors_view)\n",
    "    \n",
    "#     print(\"Check these out - they might be mislabeled or ambiguous!\")\n",
    "# else:\n",
    "#     print(\"No obvious labeling errors found - dataset looks clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Computing Embeddings - Method 2: Neural Network Features (DINOv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv3 model from FiftyOne's zoo\n",
    "# DINOv3 is a self-supervised vision transformer that produces high-quality embeddings\n",
    "model_dino = foz.load_zoo_model(\"dinov2-vits14-torch\")\n",
    "\n",
    "print(f\"Loaded DINOv3 model: {model_dino}\")\n",
    "\n",
    "# Compute DINOv3 embeddings for the entire dataset\n",
    "embeddings_dino = dataset.compute_embeddings(model_dino, batch_size=32)\n",
    "\n",
    "print(f\"DINOv3 embeddings shape: {embeddings_dino.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UMAP visualization for DINOv3 embeddings\n",
    "results_dino_umap = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings_dino,\n",
    "    brain_key=\"dinov3_umap\",\n",
    "    method=\"umap\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"DINOv3 UMAP visualization ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA visualization for DINOv3 embeddings\n",
    "# results_dino_pca = fob.compute_visualization(\n",
    "#     dataset,\n",
    "#     embeddings=embeddings_dino,\n",
    "#     brain_key=\"dinov3_pca\",\n",
    "#     method=\"pca\",\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# print(\"DINOv3 PCA visualization computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FiftyOne App to explore DINOv3 embeddings\n",
    "# You can switch between different embeddings (mobilenet_umap, dinov3_umap, dinov3_pca) \n",
    "# in the embeddings panel\n",
    "session = launch_in_browser(dataset)\n",
    "\n",
    "# In the App:\n",
    "# - Go to the embeddings panel on the right\n",
    "# - Select \"dinov3_umap\" or \"dinov3_pca\" from the dropdown\n",
    "# - Color by ground_truth.label to see how DINOv3 clusters digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embedding quality: DINOv3 vs MobileNet\n",
    "# We can compare by looking at how well the embeddings separate different digit classes\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get labels for all samples\n",
    "labels = [sample.ground_truth.label for sample in dataset]\n",
    "label_to_int = {str(i): i for i in range(10)}\n",
    "labels_int = [label_to_int[label[0]] for label in labels]\n",
    "\n",
    "# Compute silhouette scores (higher is better, range: -1 to 1)\n",
    "# Measures how similar samples are to their own cluster vs other clusters\n",
    "score_mobilenet = silhouette_score(embeddings_nn, labels_int, metric='cosine')\n",
    "score_dino = silhouette_score(embeddings_dino, labels_int, metric='cosine')\n",
    "\n",
    "print(\"Embedding Quality Comparison (Silhouette Score):\")\n",
    "print(f\"  MobileNet: {score_mobilenet:.4f}\")\n",
    "print(f\"  DINOv3:    {score_dino:.4f}\")\n",
    "print(f\"\\nHigher scores indicate better cluster separation.\")\n",
    "print(f\"DINOv3 is {'better' if score_dino > score_mobilenet else 'worse'} than MobileNet by {abs(score_dino - score_mobilenet):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training a Classifier Head on Embeddings\n",
    "\n",
    "Now let's train a simple classifier on top of the frozen DINOv3 embeddings. This demonstrates how well the embeddings capture semantic information about the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dino.shape, embeddings_nn.shape\n",
    "# ((5000, 384), (5000, 1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings and labels\n",
    "# X = embeddings_dino  # DINOv3 embeddings\n",
    "X = embeddings_nn  # MobileNet embeddings\n",
    "y = np.array([sample.ground_truth.label for sample in dataset])\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels: {np.unique(y)}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% validation, 15% test\n",
    "\n",
    "train_indices, temp_indices, y_train, y_temp = train_test_split(\n",
    "    np.arange(X.shape[0]), y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "val_indices, test_indices, y_val, y_test = train_test_split(\n",
    "    temp_indices, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test = X[train_indices], X[val_indices], X[test_indices]\n",
    "y_train, y_val, y_test = y_encoded[train_indices], y_encoded[val_indices], y_encoded[test_indices]\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP classifier head\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = X_train.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model_classifier = ClassifierHead(embedding_dim, hidden_dim=256, num_classes=num_classes)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model_classifier)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_classifier.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "num_epochs = 50\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "early_stop_patience = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_classifier, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(model_classifier, val_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        best_model_state = model_classifier.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model_classifier.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "ax1.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accs, label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(val_accs, label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"Final validation accuracy: {val_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "model_classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model_classifier(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = validate(model_classifier, test_loader, criterion)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - DINOv3 Classifier on MNIST')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(all_labels, all_preds, \n",
    "                          target_names=label_encoder.classes_,\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the misclassified training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_sample_mask = (all_labels != all_preds)\n",
    "mismatch_sample_ids = np.argwhere(mismatch_sample_mask).flatten()\n",
    "\n",
    "# map back to voxel dataset indixes\n",
    "full_dataset_mismatch_test_sample_ids = test_indices[mismatch_sample_ids]\n",
    "\n",
    "for idx, sample in enumerate(dataset):\n",
    "    if idx in full_dataset_mismatch_test_sample_ids:\n",
    "        sample[\"misclassified_test_sample\"] = True\n",
    "    else:\n",
    "        sample[\"misclassified_test_sample\"] = False\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = launch_in_browser(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: NN Classifier Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEmbedding dimension: {embedding_dim}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Final test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model_classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playground-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
